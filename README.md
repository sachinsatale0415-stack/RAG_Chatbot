# ğŸš€ LLM-Powered Multimodal RAG Chatbot

An intelligent **Retrieval-Augmented Generation (RAG) based chatbot** that answers user queries by retrieving relevant information from documents and generating responses strictly grounded in the retrieved context â€” minimizing hallucinations and improving answer accuracy.

ğŸ‘¨â€ğŸ’» **Developed by:** Sachin Satale  

ğŸ“ **Guided by:** Prof. Veena Sarda  


## ğŸŒŸ Overview

This project implements a Multimodal RAG (Retrieval-Augmented Generation) pipeline that enhances LLM responses with external knowledge retrieval.

Instead of relying only on the modelâ€™s memory, the chatbot:

1. Extracts knowledge from documents (PDF, TXT, Images)

2. Converts them into embeddings

3. Stores them in a vector database

4. Retrieves relevant chunks during queries

5. Generates grounded answers using an LLM


## âœ… Key Benefits

âœ” Reduces hallucinations

âœ” Context-aware answers

âœ” Supports multiple file formats

âœ” Scalable for enterprise use

âœ” Works on private/custom datasets


## âœ¨ Features

â€¢ ğŸ“„ PDF / TXT document ingestion

â€¢ ğŸ” Semantic search with vector embeddings

â€¢ ğŸ§  LLM-powered answer generation

â€¢ ğŸ’¬ Conversational memory (chat history aware)

â€¢ âš¡ Fast retrieval using vector database

â€¢ ğŸŒ Web UI chatbot interface

## ğŸ—ï¸ Architecture

User Query
   â†“
Retriever (Vector DB)
   â†“
Relevant Context
   â†“
LLM (RAG Prompting)
   â†“
Generated Answer


## Detailed Flow

1. Documents uploaded

2. Text extraction (PDF/OCR)

3. Chunking

4. Embedding generation

5. Stored in Vector DB

6. User query â†’ embedding

7. Top-K similarity search

8. Context + Query â†’ LLM

9. Final grounded answer
